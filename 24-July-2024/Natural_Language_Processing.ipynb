{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP stands for Natural Language Processing \n",
    "Ham computer ko hamari language jaise Hindi , English etc ki understanding dete h and uske according respond krta hai vo\n",
    "\n",
    "Document : Each row in dataset is called document\n",
    "\n",
    "Corpus : Collection of documents( i.e all rows )is called corpus\n",
    "\n",
    "Vocabulary : Unique words in Corpus\n",
    "\n",
    "Segmentation : Breaking multiple sentences into single individual sentences.\n",
    "\n",
    "Tokenization : Process of breaking sentences into words is called as tokenization and words are called as tokens\n",
    "\n",
    "StopWords : Common words used in any language are called stopwords.\n",
    "\n",
    "Stemming : Process of removing or replacing suffixes of words to get the root/base word e.x Running is converted into 'Run'\n",
    "\n",
    "Lemmatization : Process of removing or replacing suffixes of word to get the root or base word is called Lemmatization.Here, words have dictionary meaning .\n",
    "\n",
    "NER Tagging : Process of adding tags to each word like person, place, currency, etc. is called Name Entity Recognition Tagging.\n",
    "\n",
    "POS Tagging : Process of adding Part Of Speech tags to each word is call POS tagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : What is the STEP by step guide to invest In share market in India?\n",
      "Lowercase sentence : what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What is the STEP by step guide to invest In share market in India?\"\n",
    "\n",
    "sentence_lower=str(sentence).lower()\n",
    "\n",
    "print(f\"Original sentence : {sentence}\")\n",
    "print(f\"Lowercase sentence : {sentence_lower}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Hello Everyone , this is team Data Science ! We are got a project of AI - ML.\n",
      "Sentence without punctuations: Hello Everyone this is team Data Science We are got a project of AI ML.\n"
     ]
    }
   ],
   "source": [
    "sen1=\"Hello Everyone , this is team Data Science ! We are got a project of AI - ML.\"\n",
    "\n",
    "without_punc = [word for word in sen1.split(\" \") if word not in list(punc)]\n",
    "\n",
    "print(f\"Original sentence : {sen1}\")\n",
    "\n",
    "print(\"Sentence without punctuations:\" , \" \".join(without_punc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Hello Everyone, this is team Data Science! We are got a project of AI-ML.\n",
      "Sentence without punctuations: Hello Everyone this is team Data Science We are got a project of AI-ML\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sen2=\"Hello Everyone, this is team Data Science! We are got a project of AI-ML.\"\n",
    "\n",
    "word=word_tokenize(sen2)\n",
    "\n",
    "without_punc1 = [i for i in word if i not in list(punc)]\n",
    "\n",
    "print(f\"Original sentence : {sen2}\")\n",
    "\n",
    "print(\"Sentence without punctuations:\",\" \".join(without_punc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : this sentence may contain @ some special symbols liks # $ 123 and numbers from 0 to 9\n",
      "Modified sentence : this sentence may contain some special symbols liks and numbers from to \n"
     ]
    }
   ],
   "source": [
    "import re    # importing regular expressions \n",
    "\n",
    "sen3 = 'this sentence may contain @ some special symbols liks # $ 123 and numbers from 0 to 9'\n",
    "\n",
    "without_sym_num = re.sub(\"[^a-zA-Z]\",\" \",sen3)\n",
    "\n",
    "without_sym_num=re.sub(\" +\",\" \",without_sym_num)  # removes the extra spaces generated\n",
    "\n",
    "print(f\"Original sentence : {sen3}\")\n",
    "\n",
    "print(f\"Modified sentence : {without_sym_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : Hello we haave reachd final staga of deta sciense\n",
      "Corrected sentence : Hello we have reached final stage of data science\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sen4 = \"Hello we haave reachd final staga of deta sciense\"\n",
    "\n",
    "tb=TextBlob(sen4)\n",
    "\n",
    "correct_sen=tb.correct()\n",
    "\n",
    "print(f\"Original Sentence : {sen4}\")\n",
    "\n",
    "print(f\"Corrected sentence : {correct_sen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : The academic discipline of artificial intelligence was established at a research workshop held at https://en.wikipedia.org/wiki/Dartmouth_College in 1956 \n",
      "Modified sentence : The academic discipline of artificial intelligence was established at a research workshop held at in 1956 \n"
     ]
    }
   ],
   "source": [
    "sen5 = \"The academic discipline of artificial intelligence was established at a research workshop held at https://en.wikipedia.org/wiki/Dartmouth_College in 1956 \"\n",
    "\n",
    "clean_text=re.sub(\"(http|https|www)\\S+\",\" \",sen5)\n",
    "clean_text=re.sub(\" +\",\" \",clean_text)\n",
    "\n",
    "print(f\"Original sentence : {sen5}\")\n",
    "\n",
    "print(f\"Modified sentence : {clean_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Hi I am Sachin.\n",
      "I am from Hanumangarh.\n",
      "Currently I am working as Data Engineer. \n",
      "Sentence tokenization : ['Hi I am Sachin.', 'I am from Hanumangarh.', 'Currently I am working as Data Engineer.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sen6 = '''Hi I am Sachin.\n",
    "I am from Hanumangarh.\n",
    "Currently I am working as Data Engineer. '''\n",
    "\n",
    "tokens = sent_tokenize(sen6)\n",
    "print(f'Original sentence : {sen6}')\n",
    "print(f'Sentence tokenization : {tokens}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-word(n-gram character) : Tokenization N -gram are continuous sequence of words or symbols or tokens in a document. In technical terms, they can be defined as the neighbouring sequences of itmes in a documment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : learn and build has six domains like data science, data analysis, salesforce, \n",
      "cloud computing, iot, mobile app development, web app development and machine learning with AI\n",
      "n-grams is : [('learn', 'and', 'build'), ('and', 'build', 'has'), ('build', 'has', 'six'), ('has', 'six', 'domains'), ('six', 'domains', 'like'), ('domains', 'like', 'data'), ('like', 'data', 'science,'), ('data', 'science,', 'data'), ('science,', 'data', 'analysis,'), ('data', 'analysis,', 'salesforce,'), ('analysis,', 'salesforce,', '\\ncloud'), ('salesforce,', '\\ncloud', 'computing,'), ('\\ncloud', 'computing,', 'iot,'), ('computing,', 'iot,', 'mobile'), ('iot,', 'mobile', 'app'), ('mobile', 'app', 'development,'), ('app', 'development,', 'web'), ('development,', 'web', 'app'), ('web', 'app', 'development'), ('app', 'development', 'and'), ('development', 'and', 'machine'), ('and', 'machine', 'learning'), ('machine', 'learning', 'with'), ('learning', 'with', 'AI')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sen7 = '''learn and build has six domains like data science, data analysis, salesforce, \n",
    "cloud computing, iot, mobile app development, web app development and machine learning with AI'''\n",
    "\n",
    "n_grams = list(ngrams(sen7.split(\" \"), 3))\n",
    "\n",
    "print(f'Original sentence : {sen7}')\n",
    "\n",
    "print(f'n-grams is : {n_grams}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords are: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"] and total stopwords are 179\n",
      "Original sentence: after a long day at work, i decided to treat myself to a nice dinner at my favorite restaurant, \n",
      "where i could relax and enjoy some time to myself, reflecting on all the hard work and effort i've put in recently, \n",
      "which has made me feel both proud and exhausted.\n",
      "Sentence without stop words: long day work , decided treat nice dinner favorite restaurant , could relax enjoy time , reflecting hard work effort 've put recently , made feel proud exhausted .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(f'Stopwords are: {stop_words} and total stopwords are {len(stop_words)}')\n",
    "\n",
    "# Original sentence\n",
    "sen8 = '''After a long day at work, I decided to treat myself to a nice dinner at my favorite restaurant, \n",
    "where I could relax and enjoy some time to myself, reflecting on all the hard work and effort I've put in recently, \n",
    "which has made me feel both proud and exhausted.'''\n",
    "\n",
    "sen8 = str.lower(sen8)\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = word_tokenize(sen8)\n",
    "\n",
    "# Remove stopwords\n",
    "without_stopwords_sen = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Join the words to form a sentence without stopwords\n",
    "sentence_without_stopwords = ' '.join(without_stopwords_sen)\n",
    "\n",
    "print(f'Original sentence: {sen8}')\n",
    "print(f'Sentence without stop words: {sentence_without_stopwords}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the research were analyz variou algorithm to improv the effici of the autom system in organ and retriev inform from extens dataset\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# sen9 = '''connect connection connections connecting connected connects connectings'''\n",
    "sen9 = 'The researchers were analyzing various algorithms to improve the efficiency of the automated systems in organizing and retrieving information from extensive datasets'\n",
    "\n",
    "porter_stem_sen = [porter.stem(word) for word in sen9.split(\" \")]\n",
    "\n",
    "print(\" \".join(porter_stem_sen))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the research were analyz various algorithm to improv the effici of the autom system in organ and retriev inform from extens dataset\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "\n",
    "sen9 = 'The researchers were analyzing various algorithms to improve the efficiency of the automated systems in organizing and retrieving information from extensive datasets'\n",
    "\n",
    "snowball_stem_sen = [snowball.stem(word) for word in sen9.split(\" \")]\n",
    "\n",
    "print(\" \".join(snowball_stem_sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The happily running child were organizing their swiftly moving toy and enjoying playful activites\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "sen10 = 'The happily running children were organizing their swiftly moving toys and enjoying playful activites'\n",
    "\n",
    "sent_lemma = [lemma.lemmatize(word,'n') for word in sen10.split(' ')]\n",
    "\n",
    "print(\" \".join(sent_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The happily running child were organizing their swiftly moving toy and enjoying playful activity\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "sen11 = 'The happily running children were organizing their swiftly moving toys and enjoying playful activities'\n",
    "\n",
    "# Create a TextBlob object\n",
    "sent = TextBlob(sen11)\n",
    "\n",
    "# Lemmatize each word in the sentence\n",
    "textblob_lemma = [Word(w).lemmatize() for w in sent.words]\n",
    "\n",
    "print(\" \".join(textblob_lemma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word :  The || POS :  DET || Explanation :  determiner\n",
      "Word :  happily || POS :  ADV || Explanation :  adverb\n",
      "Word :  running || POS :  VERB || Explanation :  verb\n",
      "Word :  children || POS :  NOUN || Explanation :  noun\n",
      "Word :  were || POS :  AUX || Explanation :  auxiliary\n",
      "Word :  organizing || POS :  VERB || Explanation :  verb\n",
      "Word :  their || POS :  PRON || Explanation :  pronoun\n",
      "Word :  swiftly || POS :  ADV || Explanation :  adverb\n",
      "Word :  moving || POS :  VERB || Explanation :  verb\n",
      "Word :  toys || POS :  NOUN || Explanation :  noun\n",
      "Word :  and || POS :  CCONJ || Explanation :  coordinating conjunction\n",
      "Word :  enjoying || POS :  VERB || Explanation :  verb\n",
      "Word :  playful || POS :  ADJ || Explanation :  adjective\n",
      "Word :  activities || POS :  NOUN || Explanation :  noun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('The happily running children were organizing their swiftly moving toys and enjoying playful activities')\n",
    "\n",
    "for i in doc: \n",
    "    print(\"Word : \", i.text, \"||\",\"POS : \" , i.pos_,\"||\",\"Explanation : \",spacy.explain(i.pos_))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')  # You might also need this resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('name', 'NN')\n",
      "('Sachin', 'NNP')\n",
      "('I', 'PRP')\n",
      "('working', 'VBG')\n",
      "Entity  : Microsoft Bangalore || Type : PERSON\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "# 26-July-2024's class \n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence_en = stopwords.words('english')\n",
    "\n",
    "sen12 = 'my name is Sachin and I am working at Microsoft in Bangalore.'\n",
    "\n",
    "tokens = [i for i in word_tokenize(sen12) if i not in sentence_en]\n",
    "\n",
    "entities = nltk.ne_chunk(nltk.pos_tag(tokens))\n",
    "\n",
    "for entity in entities : \n",
    "    if hasattr(entity,'label'):\n",
    "        print(f\"Entity  : {' '.join(c[0] for c in entity)} || Type : {entity.label()}\")\n",
    "    else:\n",
    "        print(entity)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India Inc ORG\n",
      "Union Budget’s ORG\n",
      "July 23 DATE\n",
      "Finance ORG\n",
      "Nirmala Sitharaman PERSON\n",
      "February DATE\n",
      "Finance ORG\n",
      "Rs 2.66 PRODUCT\n",
      "Rs 17,500 PRODUCT\n",
      "Rs 50,000 PRODUCT\n",
      "4.9 percent PERCENT\n",
      "2024-25 DATE\n",
      "5.1 percent PERCENT\n",
      "Budget ORG\n",
      "4.9 percent PERCENT\n",
      "4.5 percent PERCENT\n",
      "next year DATE\n",
      "Sajjan Jindal PERSON\n",
      "JSW Group ORG\n",
      "X. ORG\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "news = '''India Inc has welcomed 2024 Union Budget’s focus on continued infrastructure push, tax cuts and rationalisation, increased focus on rural spending, sharp focus on generating more employment and skilling initiatives, while also incentivising domestic manufacturing.\n",
    "\n",
    "On July 23, Finance Minister Nirmala Sitharaman stayed the course on the government’s infra capex target of Rs 11.11 trillion announced in the interim budget in February. The Finance Minister announced a slew of measures to boost rural economy and consumption, including a provision of Rs 2.66 lakh crore for rural development, and changes to income tax slabs for the new tax regime that will help the common man save up to Rs 17,500 in income tax. The FM also increased the standard deduction for salaried employees from Rs 50,000 to Rs 75,000.\n",
    "\n",
    "The FM, however, did not lose focus on fiscal prudence, and targeted a fiscal deficit of 4.9 percent of the GDP for 2024-25, significantly lower than the target of 5.1 percent pegged in the interim Budget.\n",
    "\n",
    "“Glad to see the Union budget's commitment to fiscal discipline and infrastructure development. A fiscal deficit of 4.9 percent, with a target of 4.5 percent next year, shows our economic prudence. Prioritising employment, skilling, MSMEs, and the middle class is crucial for building a strong, resilient economy,” said Sajjan Jindal, Chairman, JSW Group in a post on social media platform X.'''\n",
    "\n",
    "doc = nlp(news)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text,entity.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
