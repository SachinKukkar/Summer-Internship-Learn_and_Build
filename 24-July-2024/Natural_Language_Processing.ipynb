{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP stands for Natural Language Processing \n",
    "Ham computer ko hamari language jaise Hindi , English etc ki understanding dete h and uske according respond krta hai vo\n",
    "\n",
    "Document : Each row in dataset is called document\n",
    "\n",
    "Corpus : Collection of documents( i.e all rows )is called corpus\n",
    "\n",
    "Vocabulary : Unique words in Corpus\n",
    "\n",
    "Segmentation : Breaking multiple sentences into single individual sentences.\n",
    "\n",
    "Tokenization : Process of breaking sentences into words is called as tokenization and words are called as tokens\n",
    "\n",
    "StopWords : Common words used in any language are called stopwords.\n",
    "\n",
    "Stemming : Process of removing or replacing suffixes of words to get the root/base word e.x Running is converted into 'Run'\n",
    "\n",
    "Lemmatization : Process of removing or replacing suffixes of word to get the root or base word is called Lemmatization.Here, words have dictionary meaning .\n",
    "\n",
    "NER Tagging : Process of adding tags to each word like person, place, currency, etc. is called Name Entity Recognition Tagging.\n",
    "\n",
    "POS Tagging : Process of adding Part Of Speech tags to each word is call POS tagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : What is the STEP by step guide to invest In share market in India?\n",
      "Lowercase sentence : what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What is the STEP by step guide to invest In share market in India?\"\n",
    "\n",
    "sentence_lower=str(sentence).lower()\n",
    "\n",
    "print(f\"Original sentence : {sentence}\")\n",
    "print(f\"Lowercase sentence : {sentence_lower}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Hello Everyone , this is team Data Science ! We are got a project of AI - ML.\n",
      "Sentence without punctuations: Hello Everyone this is team Data Science We are got a project of AI ML.\n"
     ]
    }
   ],
   "source": [
    "sen1=\"Hello Everyone , this is team Data Science ! We are got a project of AI - ML.\"\n",
    "\n",
    "without_punc = [word for word in sen1.split(\" \") if word not in list(punc)]\n",
    "\n",
    "print(f\"Original sentence : {sen1}\")\n",
    "\n",
    "print(\"Sentence without punctuations:\" , \" \".join(without_punc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sachi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : Hello Everyone, this is team Data Science! We are got a project of AI-ML.\n",
      "Sentence without punctuations: Hello Everyone this is team Data Science We are got a project of AI-ML\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sen2=\"Hello Everyone, this is team Data Science! We are got a project of AI-ML.\"\n",
    "\n",
    "word=word_tokenize(sen2)\n",
    "\n",
    "without_punc1 = [i for i in word if i not in list(punc)]\n",
    "\n",
    "print(f\"Original sentence : {sen2}\")\n",
    "\n",
    "print(\"Sentence without punctuations:\",\" \".join(without_punc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : this sentence may contain @ some special symbols liks # $ 123 and numbers from 0 to 9\n",
      "Modified sentence : this sentence may contain some special symbols liks and numbers from to \n"
     ]
    }
   ],
   "source": [
    "import re    # importing regular expressions \n",
    "\n",
    "sen3 = 'this sentence may contain @ some special symbols liks # $ 123 and numbers from 0 to 9'\n",
    "\n",
    "without_sym_num = re.sub(\"[^a-zA-Z]\",\" \",sen3)\n",
    "\n",
    "without_sym_num=re.sub(\" +\",\" \",without_sym_num)  # removes the extra spaces generated\n",
    "\n",
    "print(f\"Original sentence : {sen3}\")\n",
    "\n",
    "print(f\"Modified sentence : {without_sym_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : Hello we haave reachd final staga of deta sciense\n",
      "Corrected sentence : Hello we have reached final stage of data science\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sen4 = \"Hello we haave reachd final staga of deta sciense\"\n",
    "\n",
    "tb=TextBlob(sen4)\n",
    "\n",
    "correct_sen=tb.correct()\n",
    "\n",
    "print(f\"Original Sentence : {sen4}\")\n",
    "\n",
    "print(f\"Corrected sentence : {correct_sen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence : The academic discipline of artificial intelligence was established at a research workshop held at https://en.wikipedia.org/wiki/Dartmouth_College in 1956 \n",
      "Modified sentence : The academic discipline of artificial intelligence was established at a research workshop held at in 1956 \n"
     ]
    }
   ],
   "source": [
    "sen5 = \"The academic discipline of artificial intelligence was established at a research workshop held at https://en.wikipedia.org/wiki/Dartmouth_College in 1956 \"\n",
    "\n",
    "clean_text=re.sub(\"(http|https|www)\\S+\",\" \",sen5)\n",
    "clean_text=re.sub(\" +\",\" \",clean_text)\n",
    "\n",
    "print(f\"Original sentence : {sen5}\")\n",
    "\n",
    "print(f\"Modified sentence : {clean_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
